{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85b80b3-d840-4410-b47c-a68754ff690e",
   "metadata": {},
   "source": [
    "#### Setting up dataloader with persistent_workers set to True to reduce waiting at enumerate(train_loader) line\n",
    "\n",
    "[source](https://discuss.pytorch.org/t/what-are-the-dis-advantages-of-persistent-workers/102110)\n",
    "\n",
    "With this option to false, every time your code hits a line line for sample in dataloader:, it will create a brand new set of workers to do this loading and will kill them on exit.\n",
    "Meaning that if you have multiple dataloaders, the workers will be killed when you are done with one instantly.\n",
    "\n",
    "If you make them persist, these workers will stay around (with their state) waiting for another call into that dataloader.\n",
    "\n",
    "Setting this to True will improve performances when you call into the dataloader multiple times in a row (as creating the workers is expensive). But it also means that the dataloader will have some persistent state even when it is not used (which can use some RAM depending on your dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8e1be6-494d-4ba0-98fe-dbecea989445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from util import TwoCropTransform, AverageMeter\n",
    "\n",
    "batch_size = 512\n",
    "num_workers = 12\n",
    "img_size = 32\n",
    "\n",
    "def set_loader(batch_size, num_workers, img_size):\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "    normalize = transforms.Normalize(mean=mean, std=std)\n",
    "    # data augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(size=(img_size, img_size)),\n",
    "        transforms.RandomResizedCrop(size=img_size, scale=(0.2, 1.)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([\n",
    "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "        ], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=img_size//20*2+1, sigma=(0.1, 2.0))], p=0.5 if img_size>32 else 0.0),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='../data',\n",
    "                                     transform=TwoCropTransform(train_transform),\n",
    "                                     download=True)\n",
    "\n",
    "    train_sampler = None\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=(train_sampler is None),\n",
    "        num_workers=num_workers, pin_memory=True, sampler=train_sampler,\n",
    "        persistent_workers=True )\n",
    "\n",
    "    return train_loader\n",
    "\n",
    "train_loader = set_loader(batch_size, num_workers, img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c50f3e-7193-49df-a791-c9d9dd27c3f8",
   "metadata": {},
   "source": [
    "### On SupConResNet architecture\n",
    "#### ReLU(inplace=True)\n",
    "inplace=True means that it will modify the input directly, without allocating any additional output. It can sometimes slightly decrease the memory usage, but may not always be a valid operation (because the original input is destroyed). However, if you don’t see an error, it means that your use case is valid. [source](https://discuss.pytorch.org/t/whats-the-difference-between-nn-relu-and-nn-relu-inplace-true/948/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5335c0ae-5b96-4c04-92e1-2ab0578a9453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "SupConResNet                                  [512, 128]                --\n",
       "├─ResNet: 1-1                                 [512, 512]                --\n",
       "│    └─Conv2d: 2-1                            [512, 64, 32, 32]         1,728\n",
       "│    └─BatchNorm2d: 2-2                       [512, 64, 32, 32]         128\n",
       "│    └─Sequential: 2-3                        [512, 64, 32, 32]         --\n",
       "│    │    └─BasicBlock: 3-1                   [512, 64, 32, 32]         73,984\n",
       "│    │    └─BasicBlock: 3-2                   [512, 64, 32, 32]         73,984\n",
       "│    └─Sequential: 2-4                        [512, 128, 16, 16]        --\n",
       "│    │    └─BasicBlock: 3-3                   [512, 128, 16, 16]        230,144\n",
       "│    │    └─BasicBlock: 3-4                   [512, 128, 16, 16]        295,424\n",
       "│    └─Sequential: 2-5                        [512, 256, 8, 8]          --\n",
       "│    │    └─BasicBlock: 3-5                   [512, 256, 8, 8]          919,040\n",
       "│    │    └─BasicBlock: 3-6                   [512, 256, 8, 8]          1,180,672\n",
       "│    └─Sequential: 2-6                        [512, 512, 4, 4]          --\n",
       "│    │    └─BasicBlock: 3-7                   [512, 512, 4, 4]          3,673,088\n",
       "│    │    └─BasicBlock: 3-8                   [512, 512, 4, 4]          4,720,640\n",
       "│    └─AdaptiveAvgPool2d: 2-7                 [512, 512, 1, 1]          --\n",
       "├─Sequential: 1-2                             [512, 128]                --\n",
       "│    └─Linear: 2-8                            [512, 512]                262,656\n",
       "│    └─ReLU: 2-9                              [512, 512]                --\n",
       "│    └─Linear: 2-10                           [512, 128]                65,664\n",
       "├─Sequential: 1-3                             [512, 128]                --\n",
       "│    └─Linear: 2-11                           [512, 128]                16,512\n",
       "│    └─ReLU: 2-12                             [512, 128]                --\n",
       "│    └─Linear: 2-13                           [512, 128]                16,512\n",
       "===============================================================================================\n",
       "Total params: 11,530,176\n",
       "Trainable params: 11,530,176\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 284.56\n",
       "===============================================================================================\n",
       "Input size (MB): 6.29\n",
       "Forward/backward pass size (MB): 5036.83\n",
       "Params size (MB): 46.12\n",
       "Estimated Total Size (MB): 5089.25\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from networks.resnet_extended import SupConResNet\n",
    "from losses_negative_only import SupConLoss\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchinfo import summary\n",
    "\n",
    "def set_model(temp=0.5):\n",
    "    model = SupConResNet('resnet18')\n",
    "    criterion = SupConLoss(temperature=temp)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model.encoder = torch.nn.DataParallel(model.encoder)\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    return model, criterion\n",
    "\n",
    "model, criterion = set_model()\n",
    "summary(model, input_size=(batch_size, 3, img_size, img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759115bd-573c-41a1-be5b-9a93d2054e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(256, 3, img_size, img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4b7ba-8165-4b08-af7f-db94af3ebf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def set_optimizer(model, lr=0.5, momentum=0.9, weight_decay=1e-4):\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                          lr=lr,\n",
    "                          momentum=momentum,\n",
    "                          weight_decay=weight_decay)\n",
    "    return optimizer\n",
    "\n",
    "optimizer = set_optimizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca1ce4-c8bf-4a69-90ea-404e5386d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time1 = time.time()\n",
    "for idx, (images, labels) in enumerate(train_loader):\n",
    "    time2 = time.time()\n",
    "    print(idx, (images, labels), time2-time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03faaf-9f61-4af7-914e-3c64d97ae34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader:torch.utils.data.DataLoader, model:SupConResNet, criterion, optimizer, epoch:int):\n",
    "    \"\"\"\n",
    "    one epoch training\n",
    "\n",
    "    train_loader : union of current task samples and buffered samples, without any oversampling.\n",
    "    model : the new model to train (this function calls .train() on it)\n",
    "    model2 : frozen previous model, in test mode (previously called .eval() on it)\n",
    "    criterion : \n",
    "    optimizer : stochastic gradient descent for the model's parameters, with specified learning rate, momentum and weight decay\n",
    "    epoch : specifies which epoch this is, to calculate a warm-up learning rate if the epoch is in warm-up phase\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    distill = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"entering batch loop...\")\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        #concatenate images from both classes of the current task\n",
    "        images = torch.cat([images[0], images[1]], dim=0)\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"copying images and labels to CUDA memory...\")\n",
    "            images_cuda = images.cuda(non_blocking=True)\n",
    "            labels = labels.cuda(non_blocking=True)\n",
    "        bsz = labels.shape[0] #batch size\n",
    "\n",
    "        #forward pass\n",
    "        print(\"forward pass to model...\")\n",
    "        predictions, features = model(images_cuda, return_feat=True)\n",
    "        print(\"shape of images tensor : \", images.shape)\n",
    "\n",
    "        # AsymSupCon loss\n",
    "        f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n",
    "        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
    "        loss, logprob, mask = criterion(features, labels, target_labels=[i for i in range(0,10)])\n",
    "        print(\"AsymSupCon loss : \",loss)\n",
    "\n",
    "        # update metric\n",
    "        losses.update(loss.item(), bsz)\n",
    "\n",
    "        # SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # print info\n",
    "        print('Train: [{0}][{1}/{2}]\\t'\n",
    "              'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "              'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "              'loss {loss.val:.3f} ({loss.avg:.3f} {distill.avg:.3f})'.format(\n",
    "               epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
    "               data_time=data_time, loss=losses, distill=distill))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    return losses.avg, model2\n",
    "\n",
    "# train for one epoch\n",
    "time1 = time.time()\n",
    "loss, model = train(train_loader, model, criterion, optimizer, epoch=0)\n",
    "time2 = time.time()\n",
    "print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5323615-4fe5-4fc1-a45c-47d3f609e63c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
